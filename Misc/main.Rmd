---
title: "Misc"
output: html_document
bibliography:
  - bibliography.bib
---

```{=html}
<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 55px;
margin-right: 35px;
border-radius: 5px;
font-style: italic;
}

</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Machine Learning
- @Whalen2021 systematically summarized five common pitfalls of machine learning practice in genomics research, which can also generalize to machine learning practice in large. The five pitfalls include: 1) distributional difference (e.g. batch effects when contrusting training-testing datasets), 2) dependent structure (e.g. leaking structural dependent samples in the testing data), 3) confoudning (e.g. do not account confounding in the analysis)  4)data leakage (e.g. preprocess training-testing data together) 5) unbalanced classes of the outcome (e.g. one label dominates the sample). While the aurguments mainly focus on how the pitfalls leads to inaccurate estimation of model performance, it is hard/yet to assess how the pitfalls affects identifying the biological relationship and by how much. In the conclusion, the authors discussed the connection between the pitfalls and offers optimistic cautious that even though the performance is estimated wrong, the identification of biology relationships could be right. But it is researchers' responsibility to be aware.

<strong>Personal Comments</strong> <br> 
As the authors mentioned in the article that ML heavily relies on cross-validation, where momajority of people take the randomized cross-valdiation as default and applied to any/most of the situation regardless if it is appropriate. There should be more thoughts put in planning the ML analysis, where cross-validation is a big component of it. 

Moreover, it is important to have a baseline for comparison when evaluting the performance of ML models. In the situation where we are blind to the grand truth, we can use a simple technique by fitting model with perturbed data where there should be no signal at all.
</p>


# Reproducible Research

-   @Heil2021 proposed three standards for improving reproducibility in quantitative research. The three standards describes where and how to store data, intermediate models, workflow management systems, and containerization. Moreover, the authors gave examples of incentives as a recognition of current status quo that extra efforts on reproducibility are not well recognized.

<p class="comment">

<strong>Personal Comments</strong> <br> 
1. Interesting observations: 1) Publication of data: the authors mentioned that all data should be made publicly available in the bronze standard, but acknowledge there exists private data that can't be shared. 2) The authors introduced both R and python frameworks in different section of the work. <br> 
2. The article only proposed the standards, but leave the question _how to set up an education module for this reproducibility standards_ open for discussion.
3. Software design can be also informed from this article, particularly relavent to the privacy part. Many of the R function saves the input data as part of the model output, which leaves a privacy concern. The solution here can be very easy, set the output part to be null before saving the model. However, this indeed would be something to consider in R programming.
</p>


- @Peng2020 introduced the history of reproducible research and descirbed the difference between reproducibility and replicability. The goals and benefits of reproducible research were elaborated, where case studies were delivered to demonstrate the point: reproducibility doesn't improve past data anlysis, but contributes to future data analysis. Evaluation of the current code and data sharing was given where a vision on second phase of reproducible research is described: infrastructure development.

<p class="comment">
<strong>Personal Comments</strong> <br> 
I am surprised by how many perspectives, besides replicability, the authors are able to tie reproducibility to. The inabilities of reprod' are mentioned. The authors maintain an optimistic&cautious attitude about data & code sharing.
Rhetorically, great reading material. Content-wise, case studies pose counterintuitive conclusions but make sense if you read further. I wish the authors would give more caution abt data sharing. I'm stunned reprod' research has 30 y of history. Is trust a metric of science?
</p>

# Data sets
- @Le2020 created a database contains over 400 curated datasets for machine learning and statistical models benchmarking.



# Professional Development
## Leadership
- @Satagopan2021 provided two case studies of how leadership manifest among interdisciplinary collaboration between biostatisticians/ data scientists and clinicians. Different advices were given for leaders who's a biostatistician and non-biostatistician. In general, the leader(s) of the group should set up infrastructures for clear communication and active listening. Meanwhile, they should be cognizant of the diverse needs and career path of the members, and delegate appropriate tasks.
<p class="comment">

<strong>Personal Comments</strong> <br> 
Leadership is a two-way street, particularly for statisticians. The effort of statisticians in an collaborative team has been mostly underestimated and under-appreciated. It takes generations to train the following statisticians to know how much their efforts worth, and also to influence our collaborators to acknowledge their methodological they work with.
</p>


# References
